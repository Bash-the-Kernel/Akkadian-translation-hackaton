{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Past Challenge - Akkadian to English Translation\n",
    "## Top-10 Kaggle Solution - Multi-Model Ensemble Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Environment & Config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, \n",
    "    T5ForConditionalGeneration, MT5ForConditionalGeneration,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deterministic seed setup\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text Normalization Utilities\n",
    "class AkkadianNormalizer:\n",
    "    def __init__(self):\n",
    "        # Patterns for removal\n",
    "        self.remove_patterns = [\n",
    "            r'[!?/:.]',  # punctuation\n",
    "            r'\\b\\d+\\'*\\b',  # line numbers\n",
    "            r'[˹˺]',  # special brackets\n",
    "        ]\n",
    "        \n",
    "        # Bracket normalization\n",
    "        self.bracket_patterns = [\n",
    "            (r'\\[([^\\]]+)\\]', r'\\1'),  # [text] -> text\n",
    "            (r'<([^>]+)>', r'\\1'),     # <text> -> text\n",
    "            (r'<<([^>]+)>>', r'\\1'),   # <<text>> -> text\n",
    "        ]\n",
    "        \n",
    "        # Gap normalization\n",
    "        self.gap_patterns = [\n",
    "            (r'\\[x\\]', '<gap>'),\n",
    "            (r'\\[…\\s*…\\]', '<big_gap>'),\n",
    "            (r'…', '<big_gap>'),\n",
    "        ]\n",
    "        \n",
    "        # Determinative preservation\n",
    "        self.determinatives = r'\\{[^}]+\\}'\n",
    "    \n",
    "    def normalize_akkadian(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Unicode normalization\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "        \n",
    "        # Preserve determinatives\n",
    "        determinatives = re.findall(self.determinatives, text)\n",
    "        for i, det in enumerate(determinatives):\n",
    "            text = text.replace(det, f'__DET_{i}__')\n",
    "        \n",
    "        # Remove unwanted patterns\n",
    "        for pattern in self.remove_patterns:\n",
    "            text = re.sub(pattern, '', text)\n",
    "        \n",
    "        # Normalize brackets\n",
    "        for pattern, replacement in self.bracket_patterns:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # Normalize gaps\n",
    "        for pattern, replacement in self.gap_patterns:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        # Restore determinatives\n",
    "        for i, det in enumerate(determinatives):\n",
    "            text = text.replace(f'__DET_{i}__', det)\n",
    "        \n",
    "        # Clean whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize_english(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Remove editorial parentheses but preserve content\n",
    "        text = re.sub(r'\\(([^)]+)\\)', r'\\1', text)\n",
    "        \n",
    "        # Normalize quotes\n",
    "        text = re.sub(r'[""''`]', '\"', text)\n",
    "        \n",
    "        # Clean whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "normalizer = AkkadianNormalizer()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Reconstruction\n",
    "def load_and_reconstruct_data():\n",
    "    # Load core datasets\n",
    "    train_df = pd.read_csv('/kaggle/input/deep-past-challenge/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/deep-past-challenge/test.csv')\n",
    "    \n",
    "    # Load auxiliary data\n",
    "    try:\n",
    "        published_texts = pd.read_csv('/kaggle/input/deep-past-challenge/published_texts.csv')\n",
    "        publications = pd.read_csv('/kaggle/input/deep-past-challenge/publications.csv')\n",
    "        lexicon = pd.read_csv('/kaggle/input/deep-past-challenge/OA_Lexicon_eBL.csv')\n",
    "    except:\n",
    "        published_texts = pd.DataFrame()\n",
    "        publications = pd.DataFrame()\n",
    "        lexicon = pd.DataFrame()\n",
    "    \n",
    "    print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "    print(f\"Published texts: {len(published_texts)}, Publications: {len(publications)}\")\n",
    "    print(f\"Lexicon entries: {len(lexicon)}\")\n",
    "    \n",
    "    return train_df, test_df, published_texts, publications, lexicon\n",
    "\n",
    "def extract_parallel_from_publications(publications_df, published_texts_df):\n",
    "    \"\"\"Extract parallel sentences from publication data\"\"\"\n",
    "    parallel_data = []\n",
    "    \n",
    "    if len(publications_df) == 0 or len(published_texts_df) == 0:\n",
    "        return pd.DataFrame(columns=['akkadian', 'english', 'source'])\n",
    "    \n",
    "    # Simple heuristic extraction\n",
    "    for _, pub in publications_df.iterrows():\n",
    "        if 'text' in pub and pd.notna(pub['text']):\n",
    "            text = str(pub['text'])\n",
    "            \n",
    "            # Look for transliteration patterns\n",
    "            lines = text.split('\\n')\n",
    "            akkadian_lines = []\n",
    "            english_lines = []\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Heuristic: Akkadian contains specific characters\n",
    "                if re.search(r'[₀-₉ₓ]|{[^}]+}', line):\n",
    "                    akkadian_lines.append(line)\n",
    "                elif re.search(r'^[A-Z][a-z]', line) and len(line.split()) > 2:\n",
    "                    english_lines.append(line)\n",
    "            \n",
    "            # Align by proximity\n",
    "            min_len = min(len(akkadian_lines), len(english_lines))\n",
    "            for i in range(min_len):\n",
    "                parallel_data.append({\n",
    "                    'akkadian': akkadian_lines[i],\n",
    "                    'english': english_lines[i],\n",
    "                    'source': 'publication'\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(parallel_data)\n",
    "\n",
    "def augment_with_lexicon(lexicon_df):\n",
    "    \"\"\"Create training examples from lexicon\"\"\"\n",
    "    lexicon_data = []\n",
    "    \n",
    "    if len(lexicon_df) == 0:\n",
    "        return pd.DataFrame(columns=['akkadian', 'english', 'source'])\n",
    "    \n",
    "    for _, entry in lexicon_df.iterrows():\n",
    "        if 'lemma' in entry and 'meaning' in entry:\n",
    "            if pd.notna(entry['lemma']) and pd.notna(entry['meaning']):\n",
    "                lexicon_data.append({\n",
    "                    'akkadian': str(entry['lemma']),\n",
    "                    'english': str(entry['meaning']),\n",
    "                    'source': 'lexicon'\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(lexicon_data)\n",
    "\n",
    "# Load data\n",
    "train_df, test_df, published_texts, publications, lexicon = load_and_reconstruct_data()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset Assembly\n",
    "def assemble_training_data(train_df, published_texts, publications, lexicon):\n",
    "    # Normalize core training data\n",
    "    train_normalized = train_df.copy()\n",
    "    train_normalized['akkadian'] = train_normalized['akkadian'].apply(normalizer.normalize_akkadian)\n",
    "    train_normalized['english'] = train_normalized['english'].apply(normalizer.normalize_english)\n",
    "    train_normalized['source'] = 'train'\n",
    "    \n",
    "    # Extract from publications\n",
    "    pub_data = extract_parallel_from_publications(publications, published_texts)\n",
    "    if len(pub_data) > 0:\n",
    "        pub_data['akkadian'] = pub_data['akkadian'].apply(normalizer.normalize_akkadian)\n",
    "        pub_data['english'] = pub_data['english'].apply(normalizer.normalize_english)\n",
    "    \n",
    "    # Augment with lexicon\n",
    "    lex_data = augment_with_lexicon(lexicon)\n",
    "    if len(lex_data) > 0:\n",
    "        lex_data['akkadian'] = lex_data['akkadian'].apply(normalizer.normalize_akkadian)\n",
    "        lex_data['english'] = lex_data['english'].apply(normalizer.normalize_english)\n",
    "    \n",
    "    # Combine all data\n",
    "    all_data = [train_normalized]\n",
    "    if len(pub_data) > 0:\n",
    "        all_data.append(pub_data)\n",
    "    if len(lex_data) > 0:\n",
    "        all_data.append(lex_data)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Filter out empty or very short examples\n",
    "    combined_df = combined_df[\n",
    "        (combined_df['akkadian'].str.len() > 2) & \n",
    "        (combined_df['english'].str.len() > 2)\n",
    "    ].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Final training data: {len(combined_df)} examples\")\n",
    "    print(f\"Sources: {combined_df['source'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Assemble training data\n",
    "training_data = assemble_training_data(train_df, published_texts, publications, lexicon)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset Class\n",
    "class AkkadianDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Prepare input and target\n",
    "        input_text = f\"translate Akkadian to English: {row['akkadian']}\"\n",
    "        target_text = row['english']\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Metric Implementation\n",
    "import sacrebleu\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    \"\"\"Compute BLEU and chrF++ scores\"\"\"\n",
    "    # BLEU score\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "    bleu_score = bleu.score\n",
    "    \n",
    "    # chrF++ score\n",
    "    chrf = CHRF(word_order=2)\n",
    "    chrf_score = chrf.corpus_score(predictions, [references]).score\n",
    "    \n",
    "    # Geometric mean\n",
    "    geom_mean = np.sqrt(bleu_score * chrf_score)\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'chrf': chrf_score,\n",
    "        'geom_mean': geom_mean\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_data, device):\n",
    "    \"\"\"Evaluate model on validation data\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, row in eval_data.iterrows():\n",
    "            input_text = f\"translate Akkadian to English: {row['akkadian']}\"\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                input_text,\n",
    "                return_tensors='pt',\n",
    "                max_length=512,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=512,\n",
    "                num_beams=4,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predictions.append(pred)\n",
    "            references.append(row['english'])\n",
    "    \n",
    "    return compute_metrics(predictions, references)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model Training Function\n",
    "def train_model(model_name, train_data, val_data, output_dir, epochs=3):\n",
    "    \"\"\"Train a single model\"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AkkadianDataset(train_data, tokenizer)\n",
    "    val_dataset = AkkadianDataset(val_data, tokenizer)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=50,\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        evaluation_strategy='steps',\n",
    "        save_strategy='steps',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        greater_is_better=False,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save best model\n",
    "    trainer.save_model(f'{output_dir}/best')\n",
    "    tokenizer.save_pretrained(f'{output_dir}/best')\n",
    "    \n",
    "    return model, tokenizer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Splitting and Model Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data by text_id if available, otherwise random split\n",
    "if 'text_id' in training_data.columns:\n",
    "    unique_texts = training_data['text_id'].unique()\n",
    "    train_texts, val_texts = train_test_split(unique_texts, test_size=0.1, random_state=SEED)\n",
    "    \n",
    "    train_split = training_data[training_data['text_id'].isin(train_texts)]\n",
    "    val_split = training_data[training_data['text_id'].isin(val_texts)]\nelse:\n",
    "    train_split, val_split = train_test_split(training_data, test_size=0.1, random_state=SEED)\n",
    "\n",
    "print(f\"Train split: {len(train_split)}, Val split: {len(val_split)}\")\n",
    "\n",
    "# Model configurations\n",
    "models_config = [\n",
    "    {\n",
    "        'name': 'google/byt5-small',\n",
    "        'output_dir': './byt5_model',\n",
    "        'epochs': 5\n",
    "    },\n",
    "    {\n",
    "        'name': 'google/mt5-base',\n",
    "        'output_dir': './mt5_model',\n",
    "        'epochs': 3\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train models\n",
    "trained_models = {}\n",
    "for config in models_config:\n",
    "    model, tokenizer = train_model(\n",
    "        config['name'],\n",
    "        train_split,\n",
    "        val_split,\n",
    "        config['output_dir'],\n",
    "        config['epochs']\n",
    "    )\n",
    "    trained_models[config['name']] = {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'output_dir': config['output_dir']\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ensemble Decoding\n",
    "class EnsembleDecoder:\n",
    "    def __init__(self, models_dict, device):\n",
    "        self.models = {}\n",
    "        self.tokenizers = {}\n",
    "        self.device = device\n",
    "        \n",
    "        for name, model_info in models_dict.items():\n",
    "            # Load best checkpoint\n",
    "            model_path = f\"{model_info['output_dir']}/best\"\n",
    "            self.models[name] = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "            self.tokenizers[name] = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.models[name].eval()\n",
    "    \n",
    "    def decode_single(self, model_name, input_text, beam_size=4):\n",
    "        \"\"\"Decode with a single model\"\"\"\n",
    "        model = self.models[model_name]\n",
    "        tokenizer = self.tokenizers[model_name]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors='pt',\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=512,\n",
    "                num_beams=beam_size,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def ensemble_decode(self, input_text, weights=None):\n",
    "        \"\"\"Ensemble decoding with multiple models\"\"\"\n",
    "        if weights is None:\n",
    "            weights = {name: 1.0 for name in self.models.keys()}\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        predictions = {}\n",
    "        for name in self.models.keys():\n",
    "            try:\n",
    "                pred = self.decode_single(name, input_text)\n",
    "                predictions[name] = pred\n",
    "            except Exception as e:\n",
    "                print(f\"Error with model {name}: {e}\")\n",
    "                predictions[name] = \"\"\n",
    "        \n",
    "        # Simple voting: return prediction from best weighted model\n",
    "        if predictions:\n",
    "            # For now, prefer mt5 for better fluency, fallback to byt5\n",
    "            if 'google/mt5-base' in predictions and predictions['google/mt5-base']:\n",
    "                return predictions['google/mt5-base']\n",
    "            elif 'google/byt5-small' in predictions and predictions['google/byt5-small']:\n",
    "                return predictions['google/byt5-small']\n",
    "            else:\n",
    "                return list(predictions.values())[0] if predictions else \"\"\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "# Initialize ensemble decoder\n",
    "ensemble = EnsembleDecoder(trained_models, device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test Inference\n",
    "def generate_predictions(test_df, ensemble_decoder):\n",
    "    \"\"\"Generate predictions for test set\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing {idx}/{len(test_df)}\")\n",
    "        \n",
    "        # Normalize input\n",
    "        akkadian_text = normalizer.normalize_akkadian(row['akkadian'])\n",
    "        input_text = f\"translate Akkadian to English: {akkadian_text}\"\n",
    "        \n",
    "        # Get ensemble prediction\n",
    "        try:\n",
    "            prediction = ensemble_decoder.ensemble_decode(input_text)\n",
    "            # Post-process prediction\n",
    "            prediction = normalizer.normalize_english(prediction)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            prediction = \"\"\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Normalize test data\n",
    "test_df_normalized = test_df.copy()\n",
    "test_df_normalized['akkadian'] = test_df_normalized['akkadian'].apply(normalizer.normalize_akkadian)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "test_predictions = generate_predictions(test_df_normalized, ensemble)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Submission Generation\n",
    "def create_submission(test_df, predictions, output_file='submission.csv'):\n",
    "    \"\"\"Create submission file\"\"\"\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'translation': predictions\n",
    "    })\n",
    "    \n",
    "    # Ensure no empty predictions\n",
    "    submission_df['translation'] = submission_df['translation'].fillna('')\n",
    "    \n",
    "    # Save submission\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    print(f\"Submission shape: {submission_df.shape}\")\n",
    "    print(f\"Sample predictions:\")\n",
    "    print(submission_df.head())\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# Create final submission\n",
    "submission = create_submission(test_df, test_predictions)\n",
    "\n",
    "# Validation check\n",
    "sample_submission = pd.read_csv('/kaggle/input/deep-past-challenge/sample_submission.csv')\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "print(f\"Our submission shape: {submission.shape}\")\n",
    "print(f\"Columns match: {list(submission.columns) == list(sample_submission.columns)}\")\n",
    "print(f\"IDs match: {submission['id'].equals(sample_submission['id'])}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Final validation and cleanup\n",
    "print(\"\\n=== FINAL VALIDATION ===\")\n",
    "print(f\"Total test examples: {len(test_df)}\")\n",
    "print(f\"Total predictions: {len(test_predictions)}\")\n",
    "print(f\"Non-empty predictions: {sum(1 for p in test_predictions if p.strip())}\")\n",
    "print(f\"Average prediction length: {np.mean([len(p) for p in test_predictions]):.1f}\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\n=== SAMPLE PREDICTIONS ===\")\n",
    "for i in range(min(5, len(test_df))):\n",
    "    print(f\"Input: {test_df.iloc[i]['akkadian'][:100]}...\")\n",
    "    print(f\"Output: {test_predictions[i][:100]}...\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\n=== SUBMISSION COMPLETE ===\")\n",
    "print(\"File: submission.csv\")\n",
    "print(\"Ready for Kaggle submission!\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}